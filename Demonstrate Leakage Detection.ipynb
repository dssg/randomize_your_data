{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose  \n",
    "Demonstrate how leakage can be detected using a data randomization process.  \n",
    "## Process\n",
    "Create fake data using bernoulli GLM data-generating process (DGP). Intentionally leak information and then show that this code helps detect the leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn import metrics \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(filepath, index_col = None, do_not_randomize = None):\n",
    "    '''\n",
    "    Randomize column values of a file. Each column is randomized independently.\n",
    "    \n",
    "    Inputs:\n",
    "        filepath (str): path to file to randomize; may be of type csv or txt\n",
    "        index_col (str): optional name of column to use as index; will not \n",
    "            be randomized\n",
    "        do_not_randomize (list): optional list of strings indicating names of \n",
    "            columns that should not be randomized\n",
    "    Outputs:\n",
    "        df (dataframe): dataframe representation of randomized data\n",
    "        Output file will be generated, named as original file name + \"_randomized\"\n",
    "    '''\n",
    "    \n",
    "    # Treat csv and txt differently\n",
    "    filename, file_extension = os.path.splitext(filepath)\n",
    "    if file_extension == '.csv':\n",
    "        sep = \",\"\n",
    "    elif file_extension == '.txt':\n",
    "        sep = \"\\t\"\n",
    "        \n",
    "    # Randomize column by column\n",
    "    df = pd.read_csv(filepath, sep = sep, index_col = index_col) \n",
    "    \n",
    "    if do_not_randomize:\n",
    "        cols = [c for c in df.columns if c not in do_not_randomize]\n",
    "    else:\n",
    "        cols = df.columns\n",
    "        \n",
    "    for col in cols:\n",
    "        print('... Randomizing column ' + col)\n",
    "        df[col] = np.random.permutation(df[col])\n",
    "        \n",
    "    # Print to new csv or txt\n",
    "    new_file = filename + '_randomized' + file_extension\n",
    "    df.to_csv(new_file)   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(period, n_rows, m_columns, threshold, betas, intercept):\n",
    "    '''\n",
    "    Generate fake data for a classification task, based on data generating process where:\n",
    "    - Features are drawn iid from standard normal distribution\n",
    "    - Regression equation: y = intercept + beta_1 * x1 + .. + beta_m * xm + error\n",
    "    - Error is drawn from standard normal distribution\n",
    "    - Outcome is 1 if p value from logistic function exceeds threshold, 0 otherwise\n",
    "    \n",
    "    Inputs:\n",
    "        period (int): time period\n",
    "        n_rows (int): number of observations to generate\n",
    "        m_columns (int): number of features to generate\n",
    "        betas (list): list of numerical values to use as coefficients; length must equal m_columns, \n",
    "            i.e. don't include intercept\n",
    "        intercept (int): constant number to use as intercept of regression equation\n",
    "        \n",
    "    Outputs:\n",
    "        X (dataframe): matrix of feature variables\n",
    "        y (series): outcome variable\n",
    "    '''\n",
    "    print(\"Generating {} observations...\".format(n_rows))\n",
    "    \n",
    "    # generate column names\n",
    "    colnames = []\n",
    "    for i in range(m_columns):\n",
    "        colnames.append('x' + str(i + 1))\n",
    "    \n",
    "    # generate fake data\n",
    "    df = pd.DataFrame(np.random.normal(size=(n_rows, m_columns)), columns = colnames)\n",
    "    df.insert(loc = 0, column = 'intercept', value = intercept)\n",
    "    betas = [1] + betas\n",
    "    df['z'] = np.multiply(np.array(betas), df).sum(axis=1) + np.random.normal()\n",
    "    df['pr'] = df['z'].apply(lambda x: 1/(1 + math.exp(x)))\n",
    "    df['outcome'] = np.where(df['pr'] > threshold, 1, 0)\n",
    "    df.index.name = 'idx'\n",
    "    df['period'] = period\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_logreg(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    print(\"Baseline accuracy: \" + str(round(1 - np.mean(y_train), 2)))\n",
    "    \n",
    "    # Create a logistic regression object\n",
    "    logreg = linear_model.LogisticRegression()\n",
    "\n",
    "    # Train model with training set\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions with test set\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    # Save probabilities\n",
    "    probs = logreg.predict_proba(X_test)\n",
    "\n",
    "    # Print accuracy and AUC\n",
    "    print(\"Accuracy: \" + str(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print(\"AUC Score: \" + str(metrics.roc_auc_score(y_test, probs[:, 1])))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return logreg#, X_test, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_y(df):\n",
    "    '''\n",
    "    Split data frame into X and y objects \n",
    "    '''\n",
    "    X = df.drop(['z', 'pr', 'outcome'], axis = 1)\n",
    "    y = df['outcome']\n",
    "    return X, y\n",
    "\n",
    "def logreg_pipeline(df, temporal = False):\n",
    "    '''\n",
    "    Put raw inputs through basic machine learning pipeline.\n",
    "    \n",
    "    Inputs:\n",
    "        dataframe: Pandas dataframe representing raw input file\n",
    "        temporal (booL): Indicator for whether a temporal TimeSeriesSplit\n",
    "            should be use instead of non-temporal train_test_split\n",
    "    OUtputs:\n",
    "        list of logistic regression model objects\n",
    "    '''\n",
    "    \n",
    "    # Save models\n",
    "    models = []\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    if temporal: # do temporal train-test split\n",
    "        periods = np.sort(df.period.unique())\n",
    "        splits = TimeSeriesSplit(n_splits = max(periods))\n",
    "        print(\"Time series split\")\n",
    "        for train_index, test_index in splits.split(periods):\n",
    "            print(\"Train periods: \", train_index, \"Test periods: \", test_index)\n",
    "            df_train = df[df.period.isin(train_index)]\n",
    "            df_test = df[df.period.isin(test_index)]\n",
    "            X_train, y_train = get_X_y(df_train)\n",
    "            X_test, y_test = get_X_y(df_test)\n",
    "            models.append(run_logreg(X_train, y_train, X_test, y_test))\n",
    "\n",
    "    else: # do random, non-temporal train-test split\n",
    "        print(\"Train test split -- test size = 0.33\")\n",
    "        X, y = get_X_y(df)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        models.append(run_logreg(X_train, y_train, X_test, y_test))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Stable DGP (same parameters and threshold over time)\n",
    "### Non-temporal example with stable DGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data for 5 periods with constant DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 200 observations...\n",
      "Generating 200 observations...\n",
      "Generating 200 observations...\n",
      "Generating 200 observations...\n",
      "Generating 200 observations...\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i in range(5):\n",
    "    period_df = generate_data(period = i, n_rows = 200, m_columns = 5, threshold = 0.8, betas = [1,2,3,4,5], intercept = 1)\n",
    "    frames.append(period_df)\n",
    "df = pd.concat(frames)\n",
    "df.to_csv('fake_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run original input file through pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 0.951515151515\n",
      "AUC Score: 0.993717636308\n",
      "[[186  10]\n",
      " [  6 128]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the input file through randomization process, then run it through the pipeline. Model performs no better than random, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Randomizing column intercept\n",
      "... Randomizing column x1\n",
      "... Randomizing column x2\n",
      "... Randomizing column x3\n",
      "... Randomizing column x4\n",
      "... Randomizing column x5\n",
      "... Randomizing column z\n",
      "... Randomizing column pr\n",
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.59\n",
      "Accuracy: 0.572727272727\n",
      "AUC Score: 0.489920956091\n",
      "[[182  11]\n",
      " [130   7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random = randomize('fake_data.csv', 'idx', ['period', 'outcome']) # creates csv named fake_data_randomized.csv\n",
    "logreg_pipeline(df_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try introducing an obvious case of data leakage by using the outcome column as a feature. Run this through the pipeline. Model performs well, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 1.0\n",
      "AUC Score: 1.0\n",
      "[[200   0]\n",
      " [  0 130]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['leaky_col'] = df['outcome']\n",
    "logreg_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the input file through randomization process. Again, introduce data leakage into the pipeline. Model performs better than random, thus suggesting the existence of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Randomizing column intercept\n",
      "... Randomizing column x1\n",
      "... Randomizing column x2\n",
      "... Randomizing column x3\n",
      "... Randomizing column x4\n",
      "... Randomizing column x5\n",
      "... Randomizing column z\n",
      "... Randomizing column pr\n",
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.59\n",
      "Accuracy: 1.0\n",
      "AUC Score: 1.0\n",
      "[[191   0]\n",
      " [  0 139]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random = randomize('fake_data.csv', 'idx', ['period', 'outcome']) # generates csv named \"fake_data_randomized.csv\"\n",
    "df_random['leaky_col'] = df_random['outcome']\n",
    "logreg_pipeline(df_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Regime change - intercept\n",
    "### The DGP is stable over time, then the intercept suddenly changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data for 6 periods with intercept changes in periods 2 and 4  \n",
    "Dataset is in temporal order by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 200 observations...\n",
      "Generating 200 observations...\n",
      "Generating 200 observations...\n",
      "Generating 200 observations...\n",
      "Generating 200 observations...\n",
      "Generating 200 observations...\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i in [0,1]:\n",
    "    period_df = generate_data(period = i, n_rows = 200, m_columns = 5, threshold = 0.8, betas = [1,2,3,4,5], intercept = 1)\n",
    "    frames.append(period_df)\n",
    "for i in [2,3]:\n",
    "    period_df = generate_data(period = i, n_rows = 200, m_columns = 5, threshold = 0.8, betas = [1,2,3,4,5], intercept = 2) \n",
    "    frames.append(period_df)\n",
    "for i in [4,5]:\n",
    "    period_df = generate_data(period = i, n_rows = 200, m_columns = 5, threshold = 0.8, betas = [1,2,3,4,5], intercept = 3) \n",
    "    frames.append(period_df)\n",
    "df = pd.concat(frames)\n",
    "df.to_csv('fake_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you use regular split (thus introducing leakage) vs the correct time series split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline using appropriate temporal train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series split\n",
      "Train periods:  [0] Test periods:  [1]\n",
      "Baseline accuracy: 0.76\n",
      "Accuracy: 0.745\n",
      "AUC Score: 0.98686716792\n",
      "[[105   0]\n",
      " [ 51  44]]\n",
      "Train periods:  [0 1] Test periods:  [2]\n",
      "Baseline accuracy: 0.64\n",
      "Accuracy: 0.805\n",
      "AUC Score: 0.977114121511\n",
      "[[78 38]\n",
      " [ 1 83]]\n",
      "Train periods:  [0 1 2] Test periods:  [3]\n",
      "Baseline accuracy: 0.62\n",
      "Accuracy: 0.835\n",
      "AUC Score: 0.988204456094\n",
      "[[76 33]\n",
      " [ 0 91]]\n",
      "Train periods:  [0 1 2 3] Test periods:  [4]\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 0.875\n",
      "AUC Score: 0.991779448622\n",
      "[[80 25]\n",
      " [ 0 95]]\n",
      "Train periods:  [0 1 2 3 4] Test periods:  [5]\n",
      "Baseline accuracy: 0.59\n",
      "Accuracy: 0.915\n",
      "AUC Score: 0.989298929893\n",
      "[[84 15]\n",
      " [ 2 99]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_pipeline(df, temporal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put input file through randomization process, then run it through the pipeline. Model performs no better than random, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Randomizing column intercept\n",
      "... Randomizing column x1\n",
      "... Randomizing column x2\n",
      "... Randomizing column x3\n",
      "... Randomizing column x4\n",
      "... Randomizing column x5\n",
      "... Randomizing column z\n",
      "... Randomizing column pr\n",
      "Time series split\n",
      "Train periods:  [0] Test periods:  [1]\n",
      "Baseline accuracy: 0.76\n",
      "Accuracy: 0.525\n",
      "AUC Score: 0.427468671679\n",
      "[[105   0]\n",
      " [ 95   0]]\n",
      "Train periods:  [0 1] Test periods:  [2]\n",
      "Baseline accuracy: 0.64\n",
      "Accuracy: 0.42\n",
      "AUC Score: 0.402914614122\n",
      "[[  0 116]\n",
      " [  0  84]]\n",
      "Train periods:  [0 1 2] Test periods:  [3]\n",
      "Baseline accuracy: 0.62\n",
      "Accuracy: 0.45\n",
      "AUC Score: 0.479685452163\n",
      "[[  9 100]\n",
      " [ 10  81]]\n",
      "Train periods:  [0 1 2 3] Test periods:  [4]\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 0.475\n",
      "AUC Score: 0.565614035088\n",
      "[[ 8 97]\n",
      " [ 8 87]]\n",
      "Train periods:  [0 1 2 3 4] Test periods:  [5]\n",
      "Baseline accuracy: 0.59\n",
      "Accuracy: 0.51\n",
      "AUC Score: 0.477247724772\n",
      "[[11 88]\n",
      " [10 91]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random = randomize('fake_data.csv', 'idx', ['period', 'outcome']) # creates csv named fake_data_randomized.csv\n",
    "logreg_pipeline(df_random, temporal = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce data leakage by using non-temporal train-test split. Run this through the pipeline. Model performs well, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.56\n",
      "Accuracy: 0.94696969697\n",
      "AUC Score: 0.985123966942\n",
      "[[221  10]\n",
      " [ 11 154]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the input file through randomization process. Again, introduce data leakage into the pipeline by using non-temporal train-test split. Model performs no better than random. EXPECTING TO SEE IT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Randomizing column intercept\n",
      "... Randomizing column x1\n",
      "... Randomizing column x2\n",
      "... Randomizing column x3\n",
      "... Randomizing column x4\n",
      "... Randomizing column x5\n",
      "... Randomizing column z\n",
      "... Randomizing column pr\n",
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.58\n",
      "Accuracy: 0.55303030303\n",
      "AUC Score: 0.555475113122\n",
      "[[185  36]\n",
      " [141  34]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random = randomize('fake_data.csv', 'idx', ['period', 'outcome']) # generates csv named \"fake_data_randomized.csv\"\n",
    "logreg_pipeline(df_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
