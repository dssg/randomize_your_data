{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose  \n",
    "Demonstrate how leakage can be detected using a data randomization process.  \n",
    "## Process\n",
    "Create fake data using bernoulli GLM data-generating process (DGP). Intentionally leak information and then show that this code helps detect the leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn import metrics \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(filepath, index_col = None, do_not_randomize = None):\n",
    "    '''\n",
    "    Randomize column values of a file. Each column is randomized independently.\n",
    "    \n",
    "    Inputs:\n",
    "        filepath (str): path to file to randomize; may be of type csv or txt\n",
    "        index_col (str): optional name of column to use as index; will not \n",
    "            be randomized\n",
    "        do_not_randomize (list): optional list of strings indicating names of \n",
    "            columns that should not be randomized\n",
    "    Outputs:\n",
    "        df (dataframe): dataframe representation of randomized data\n",
    "        Output file will be generated, named as original file name + \"_randomized\"\n",
    "    '''\n",
    "    \n",
    "    # Treat csv and txt differently\n",
    "    filename, file_extension = os.path.splitext(filepath)\n",
    "    if file_extension == '.csv':\n",
    "        sep = \",\"\n",
    "    elif file_extension == '.txt':\n",
    "        sep = \"\\t\"\n",
    "        \n",
    "    # Randomize column by column\n",
    "    df = pd.read_csv(filepath, sep = sep, index_col = index_col) \n",
    "    \n",
    "    if do_not_randomize:\n",
    "        cols = [c for c in df.columns if c not in do_not_randomize]\n",
    "    else:\n",
    "        cols = df.columns\n",
    "        \n",
    "    for col in cols:\n",
    "        print('... Randomizing column ' + col)\n",
    "        df[col] = np.random.permutation(df[col])\n",
    "        \n",
    "    # Print to new csv or txt\n",
    "    new_file = filename + '_randomized' + file_extension\n",
    "    df.to_csv(new_file)   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(period, n_rows, m_columns, threshold, betas, intercept):\n",
    "    '''\n",
    "    Generate fake data for a classification task, based on data generating process where:\n",
    "    - Features are drawn iid from standard normal distribution\n",
    "    - Regression equation: y = intercept + beta_1 * x1 + .. + beta_m * xm + error\n",
    "    - Error is drawn from standard normal distribution\n",
    "    - Outcome is 1 if p value from logistic function exceeds threshold, 0 otherwise\n",
    "    \n",
    "    Inputs:\n",
    "        period (int): time period\n",
    "        n_rows (int): number of observations to generate\n",
    "        m_columns (int): number of features to generate\n",
    "        betas (list): list of numerical values to use as coefficients; length must equal m_columns, \n",
    "            i.e. don't include intercept\n",
    "        intercept (int): constant number to use as intercept of regression equation\n",
    "        \n",
    "    Outputs:\n",
    "        X (dataframe): matrix of feature variables\n",
    "        y (series): outcome variable\n",
    "    '''\n",
    "    print(\"Generating {} observations...\".format(n_rows))\n",
    "    \n",
    "    # generate column names\n",
    "    colnames = []\n",
    "    for i in range(m_columns):\n",
    "        colnames.append('x' + str(i + 1))\n",
    "    \n",
    "    # generate fake data\n",
    "    df = pd.DataFrame(np.random.normal(size=(n_rows, m_columns)), columns = colnames)\n",
    "    df.insert(loc = 0, column = 'intercept', value = intercept)\n",
    "    betas = [1] + betas\n",
    "    df['z'] = np.multiply(np.array(betas), df).sum(axis=1) + np.random.normal()\n",
    "    df['pr'] = df['z'].apply(lambda x: 1/(1 + math.exp(x)))\n",
    "    df['outcome'] = np.where(df['pr'] > threshold, 1, 0)\n",
    "    df.index.name = 'idx'\n",
    "    df['period'] = period\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_logreg(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    Run logistic regression on given test and training sets        \n",
    "    '''\n",
    "    print(\"Baseline accuracy: \" + str(round(1 - np.mean(y_train), 2)))\n",
    "    \n",
    "    # Create a logistic regression object\n",
    "    logreg = linear_model.LogisticRegression()\n",
    "\n",
    "    # Train model with training set\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions with test set\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    # Save probabilities\n",
    "    probs = logreg.predict_proba(X_test)\n",
    "\n",
    "    # Print accuracy and AUC\n",
    "    print(\"Accuracy: \" + str(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print(\"AUC Score: \" + str(metrics.roc_auc_score(y_test, probs[:, 1])))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return logreg, X_test, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_y(df):\n",
    "    '''\n",
    "    Split data frame into X and y objects \n",
    "    '''\n",
    "    X = df.drop(['z', 'pr', 'outcome'], axis = 1)\n",
    "    y = df['outcome']\n",
    "    return X, y\n",
    "\n",
    "def logreg_pipeline(df, temporal = False):\n",
    "    '''\n",
    "    Put raw inputs through basic machine learning pipeline.\n",
    "    \n",
    "    Inputs:\n",
    "        dataframe: Pandas dataframe representing raw input file\n",
    "        temporal (booL): Indicator for whether a temporal TimeSeriesSplit\n",
    "            should be use instead of non-temporal train_test_split\n",
    "    OUtputs:\n",
    "        list of logistic regression model objects\n",
    "    '''\n",
    "    \n",
    "    # Save results of run_logreg\n",
    "    models = []\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    if temporal: # do temporal train-test split\n",
    "        periods = np.sort(df.period.unique())\n",
    "        splits = TimeSeriesSplit(n_splits = max(periods))\n",
    "        print(\"Time series split\")\n",
    "        for train_index, test_index in splits.split(periods):\n",
    "            print(\"Train periods: \", train_index, \"Test periods: \", test_index)\n",
    "            df_train = df[df.period.isin(train_index)]\n",
    "            df_test = df[df.period.isin(test_index)]\n",
    "            X_train, y_train = get_X_y(df_train)\n",
    "            X_test, y_test = get_X_y(df_test)\n",
    "            models.append(run_logreg(X_train, y_train, X_test, y_test))\n",
    "\n",
    "    else: # do random, non-temporal train-test split\n",
    "        print(\"Train test split -- test size = 0.33\")\n",
    "        X, y = get_X_y(df)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        models.append(run_logreg(X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Stable DGP \n",
    "### Non-temporal example with stable DGP; data leakage through outcome variable as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data for 5 periods with constant DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i in range(5):\n",
    "    period_df = generate_data(period = i, n_rows = 2000, m_columns = 5, threshold = 0.8, betas = [1,2,3,4,5], intercept = 1)\n",
    "    frames.append(period_df)\n",
    "df = pd.concat(frames)\n",
    "df.to_csv('fake_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run original input file through pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 0.976363636364\n",
      "AUC Score: 0.998165881132\n",
      "[[1911   42]\n",
      " [  36 1311]]\n"
     ]
    }
   ],
   "source": [
    "models = logreg_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the input file through randomization process, then run it through the pipeline. Model performs no better than random, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Randomizing column intercept\n",
      "... Randomizing column x1\n",
      "... Randomizing column x2\n",
      "... Randomizing column x3\n",
      "... Randomizing column x4\n",
      "... Randomizing column x5\n",
      "... Randomizing column z\n",
      "... Randomizing column pr\n",
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 0.591212121212\n",
      "AUC Score: 0.525692665258\n",
      "[[1951    0]\n",
      " [1349    0]]\n"
     ]
    }
   ],
   "source": [
    "df_random = randomize('fake_data.csv', 'idx', ['period', 'outcome']) # creates csv named fake_data_randomized.csv\n",
    "models = logreg_pipeline(df_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try introducing an obvious case of data leakage by using the outcome column as a feature. Run this through the pipeline. Model performs well, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.59\n",
      "Accuracy: 1.0\n",
      "AUC Score: 1.0\n",
      "[[1973    0]\n",
      " [   0 1327]]\n"
     ]
    }
   ],
   "source": [
    "df['leaky_col'] = df['outcome']\n",
    "models = logreg_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the input file through randomization process. Again, introduce data leakage into the pipeline. Model performs better than random, thus suggesting the existence of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Randomizing column intercept\n",
      "... Randomizing column x1\n",
      "... Randomizing column x2\n",
      "... Randomizing column x3\n",
      "... Randomizing column x4\n",
      "... Randomizing column x5\n",
      "... Randomizing column z\n",
      "... Randomizing column pr\n",
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 1.0\n",
      "AUC Score: 1.0\n",
      "[[1958    0]\n",
      " [   0 1342]]\n"
     ]
    }
   ],
   "source": [
    "df_random = randomize('fake_data.csv', 'idx', ['period', 'outcome']) # generates csv named \"fake_data_randomized.csv\"\n",
    "df_random['leaky_col'] = df_random['outcome']\n",
    "models = logreg_pipeline(df_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Stable DGP\n",
    "### Non-temporal example with stable DGP; data leakage through selection of variables that are highly correlated with outcome (i.e. proxy variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data with 1000 variables for 5 periods with constant DGP. Betas are random integers between 0 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i in range(5):\n",
    "    period_df = generate_data(period = i, n_rows = 2000, m_columns = 1000, threshold = 0.8, betas = list(np.random.randint(0, 10, size=1000)), intercept = 1)\n",
    "    frames.append(period_df)\n",
    "df = pd.concat(frames)\n",
    "df.to_csv('fake_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run original input file through pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.5\n",
      "Accuracy: 0.815454545455\n",
      "AUC Score: 0.898647837057\n",
      "[[1330  309]\n",
      " [ 300 1361]]\n"
     ]
    }
   ],
   "source": [
    "models = logreg_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute correlation of each variable with outcome variable. Select top 100 highest positively-correlated variables to use as features.\n",
    "\n",
    "Model performs worse now than when all variables are used as features. This is expected because outcome is a function of all 1000 variables in true relationship, so predictive power of 100 variables is not as high as 1000 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = []\n",
    "for i in range(1000):\n",
    "    corr_coeff = df['x'+str(i + 1)].corr(df.outcome)\n",
    "    correlations.append((i, corr_coeff))\n",
    "top_100 = sorted(correlations, key=lambda x: x[1])[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\"x\"+str(x[0]) for x in top_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['intercept'] + indices + ['z', 'pr', 'outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.5\n",
      "Accuracy: 0.568787878788\n",
      "AUC Score: 0.601245048603\n",
      "[[896 760]\n",
      " [663 981]]\n"
     ]
    }
   ],
   "source": [
    "models = logreg_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Regime change - intercept \n",
    "### DGP is stable over time, then the intercept suddenly changes; data leakage through non-temporal train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data for 5 periods with intercept change in period 2  \n",
    "Dataset is in temporal order by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n",
      "Generating 2000 observations...\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i in [0,1]:\n",
    "    period_df = generate_data(period = i, n_rows = 2000, m_columns = 5, threshold = 0.8, betas = [1,2,3,4,5], intercept = 1)\n",
    "    frames.append(period_df)\n",
    "for i in [2,3,4]:\n",
    "    period_df = generate_data(period = i, n_rows = 2000, m_columns = 5, threshold = 0.8, betas = [1,2,3,4,5], intercept = 2) \n",
    "    frames.append(period_df)\n",
    "df = pd.concat(frames)\n",
    "df.to_csv('fake_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline using appropriate temporal train-test split. Models does well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series split\n",
      "Train periods:  [0] Test periods:  [1]\n",
      "Baseline accuracy: 0.54\n",
      "Accuracy: 0.9555\n",
      "AUC Score: 0.99991262111\n",
      "[[1076   89]\n",
      " [   0  835]]\n",
      "Train periods:  [0 1] Test periods:  [2]\n",
      "Baseline accuracy: 0.56\n",
      "Accuracy: 0.939\n",
      "AUC Score: 0.999934664619\n",
      "[[1256  122]\n",
      " [   0  622]]\n",
      "Train periods:  [0 1 2] Test periods:  [3]\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 0.9\n",
      "AUC Score: 0.999912029813\n",
      "[[1301    0]\n",
      " [ 200  499]]\n",
      "Train periods:  [0 1 2 3] Test periods:  [4]\n",
      "Baseline accuracy: 0.62\n",
      "Accuracy: 0.889\n",
      "AUC Score: 0.999962059249\n",
      "[[1339  222]\n",
      " [   0  439]]\n"
     ]
    }
   ],
   "source": [
    "models = logreg_pipeline(df, temporal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put input file through randomization process, then run it through the pipeline. Model performs no better than random (0 true positives), as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Randomizing column intercept\n",
      "... Randomizing column x1\n",
      "... Randomizing column x2\n",
      "... Randomizing column x3\n",
      "... Randomizing column x4\n",
      "... Randomizing column x5\n",
      "... Randomizing column z\n",
      "... Randomizing column pr\n",
      "Time series split\n",
      "Train periods:  [0] Test periods:  [1]\n",
      "Baseline accuracy: 0.54\n",
      "Accuracy: 0.5865\n",
      "AUC Score: 0.523102464599\n",
      "[[1133   32]\n",
      " [ 795   40]]\n",
      "Train periods:  [0 1] Test periods:  [2]\n",
      "Baseline accuracy: 0.56\n",
      "Accuracy: 0.689\n",
      "AUC Score: 0.504981822764\n",
      "[[1378    0]\n",
      " [ 622    0]]\n",
      "Train periods:  [0 1 2] Test periods:  [3]\n",
      "Baseline accuracy: 0.6\n",
      "Accuracy: 0.6505\n",
      "AUC Score: 0.49058554056\n",
      "[[1301    0]\n",
      " [ 699    0]]\n",
      "Train periods:  [0 1 2 3] Test periods:  [4]\n",
      "Baseline accuracy: 0.62\n",
      "Accuracy: 0.7805\n",
      "AUC Score: 0.509337072929\n",
      "[[1561    0]\n",
      " [ 439    0]]\n"
     ]
    }
   ],
   "source": [
    "df_random = randomize('fake_data.csv', 'idx', ['period', 'outcome']) # creates csv named fake_data_randomized.csv\n",
    "models = logreg_pipeline(df_random, temporal = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, introduce data leakage by using non-temporal train-test split. Run this through the pipeline. Model performs well, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.65\n",
      "Accuracy: 0.969090909091\n",
      "AUC Score: 0.996620424671\n",
      "[[2096   54]\n",
      " [  48 1102]]\n"
     ]
    }
   ],
   "source": [
    "models = logreg_pipeline(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe model performance in each period. No obvious trends..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when test period is 0: 0.9939577039274925\n",
      "Accuracy when test period is 1: 0.9967159277504105\n",
      "Accuracy when test period is 2: 0.9667149059334298\n",
      "Accuracy when test period is 3: 0.9327354260089686\n",
      "Accuracy when test period is 4: 0.9581464872944694\n"
     ]
    }
   ],
   "source": [
    "logreg, X_test, y_test, y_pred = models[0]\n",
    "\n",
    "periods = np.sort(df.period.unique())\n",
    "\n",
    "test_df = X_test\n",
    "test_df['y_test'] = y_test\n",
    "test_df['y_pred'] = y_pred\n",
    "\n",
    "for i in periods:\n",
    "    df_i = test_df[test_df.period == i]\n",
    "    acc = metrics.accuracy_score(df_i.y_test, df_i.y_pred)\n",
    "    print(\"Accuracy when test period is {}: {}\".format(i, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the input file through randomization process, randomizing all columns except period and outcome. Again, introduce data leakage into the pipeline by using non-temporal train-test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Randomizing column intercept\n",
      "... Randomizing column x1\n",
      "... Randomizing column x2\n",
      "... Randomizing column x3\n",
      "... Randomizing column x4\n",
      "... Randomizing column x5\n",
      "... Randomizing column z\n",
      "... Randomizing column pr\n",
      "Train test split -- test size = 0.33\n",
      "Baseline accuracy: 0.65\n",
      "Accuracy: 0.648787878788\n",
      "AUC Score: 0.576077918704\n",
      "[[2134   17]\n",
      " [1142    7]]\n"
     ]
    }
   ],
   "source": [
    "df_random = randomize('fake_data.csv', 'idx', ['period', 'outcome']) # generates csv named \"fake_data_randomized.csv\"\n",
    "models = logreg_pipeline(df_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe model performance in each period. We can see that this data leakage causes the higher accuracy in later test periods after the data generating process changes, because information about the future has been leaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when test period is 0: 0.5446153846153846\n",
      "Accuracy when test period is 1: 0.593841642228739\n",
      "Accuracy when test period is 2: 0.7027027027027027\n",
      "Accuracy when test period is 3: 0.6384615384615384\n",
      "Accuracy when test period is 4: 0.7619738751814223\n"
     ]
    }
   ],
   "source": [
    "logreg, X_test, y_test, y_pred = models[0]\n",
    "\n",
    "periods = np.sort(df.period.unique())\n",
    "\n",
    "test_df = X_test\n",
    "test_df['y_test'] = y_test\n",
    "test_df['y_pred'] = y_pred\n",
    "\n",
    "for i in periods:\n",
    "    df_i = test_df[test_df.period == i]\n",
    "    acc = metrics.accuracy_score(df_i.y_test, df_i.y_pred)\n",
    "    print(\"Accuracy when test period is {}: {}\".format(i, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
