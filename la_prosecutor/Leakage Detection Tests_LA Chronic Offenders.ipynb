{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leakage Detection Tests\n",
    "\n",
    "A series of tests for leakage, demonstrated on the [Los Angeles Chronic Offenders Leakage](https://github.com/dssg/la_prosecutor) project  \n",
    "Requires a credentials.py file defining the following variables: dbname, user, host, password, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from catwalk.db import connect\n",
    "from catwalk.storage import FSModelStorageEngine\n",
    "from sqlalchemy import create_engine\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "from credentials import dbname, user, host, password, port\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "warnings.simplefilter('ignore')\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(statement, dbname, user, host, password, port, isolation = False, results = True):\n",
    "    \"\"\"\n",
    "    Use psycopg2 to execute PostgreSQL queries\n",
    "    \n",
    "    Input:\n",
    "        statement (str): SQL statement to run in database\n",
    "        dbname, user, host, password (str): database credentials\n",
    "        isolation (bool): indicator for whether to change isolation level to autocommit; True for queries that cannot be run \n",
    "            from within a transation (see https://wiki.postgresql.org/wiki/Psycopg2_Tutorial)\n",
    "        results (bool): indicator for whether the query is expected to output results;\n",
    "            for example, True for SELECT statements and False for CREATE TABLE statements\n",
    "    \n",
    "    Output:\n",
    "        relation (dataframe): query results or empty dataframe if results = False\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\"dbname={} user={} host={} password={}\".format(dbname, user, host, password))\n",
    "    cur = conn.cursor()\n",
    "    if isolation:\n",
    "        conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cur.execute(statement)\n",
    "    relation = pd.DataFrame()\n",
    "    if results:\n",
    "        results = cur.fetchall()\n",
    "        colnames = [desc[0] for desc in cur.description]\n",
    "        relation = pd.DataFrame(results, columns=colnames)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(df, do_not_randomize = None, seed=0):\n",
    "    \"\"\"\n",
    "    Randomize column values of a file. Each column is randomized independently.\n",
    "    \n",
    "    Inputs:\n",
    "        df (dataframe): dataframe to randomize\n",
    "        do_not_randomize (list): optional list of strings indicating names of \n",
    "            columns that should not be randomized\n",
    "    Outputs:\n",
    "        df (dataframe): dataframe of randomized data\n",
    "    \"\"\"     \n",
    "    df_random = df.copy()\n",
    "    if do_not_randomize:\n",
    "        cols = [c for c in df.columns if c not in do_not_randomize]\n",
    "    else:\n",
    "        cols = df.columns\n",
    "        \n",
    "    for col in cols:\n",
    "        #print('\\t\\tRandomizing column ' + col)\n",
    "        np.random.seed(seed)\n",
    "        df_random[col] = np.random.permutation(df_random[col])\n",
    "\n",
    "    return df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://overlaid.net/2016/02/08/replace-words-in-files-or-strings-using-python/\n",
    "def do_replacement(base_text, word_map):\n",
    "    \"\"\"\n",
    "    Helper function for replace_words_in_file. Make replacements in base_text, as \n",
    "    indicated in  word_map.\n",
    "    \"\"\"\n",
    "    for key, val in word_map.items():\n",
    "        base_text = base_text.replace(key, val)\n",
    "    return base_text\n",
    "\n",
    "def replace_words_in_file(read_from, write_to, word_map):\n",
    "    \"\"\"\n",
    "    Create copy of a file with certain words replaced\n",
    "    \n",
    "    Inputs\n",
    "        read_from: name of file to read from\n",
    "        write_to: name of new file to be created\n",
    "        word_map: dictionary of mappings between words and their replacements\n",
    "            (e.g. {'old_word': 'new_word'})\n",
    "    Outputs\n",
    "        None. Will create a new file with the name given in write_to\n",
    "        \n",
    "    \"\"\"\n",
    "    print('Generating file ', write_to)\n",
    "    # Open your desired file as 't' and read the lines into string 'tempstr'\n",
    "    t = open(read_from, 'r')\n",
    "    tempstr = t.read()\n",
    "    t.close()\n",
    "\n",
    "    # Using the \"replace_words\" function, we'll pass in our tempstr to be used as the base, \n",
    "    # and our device_values to be used as replacement.  \n",
    "    output = do_replacement(tempstr, word_map)\n",
    "\n",
    "    # Write out the new config file\n",
    "    fout = open(write_to, 'w')\n",
    "    fout.write(output)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_filename(filename, suffix, replacement_map=None):\n",
    "    \"\"\"\n",
    "    Output a new filename (str) given a filename and suffix to append. Assumes file extension at the end is separated by a period.\n",
    "    \"\"\"\n",
    "    k = filename.rfind(\".\")\n",
    "    new_filename = filename[:k] + suffix + '.' + filename[k+1:]\n",
    "    if replacement_map:\n",
    "        new_filename = do_replacement(new_filename, replacement_map)\n",
    "    return new_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Randomize the input data\n",
    "\n",
    "A number of preprocessing steps were required for this project, because raw cases and bookings data did not come with unique identifiers for the individuals involved. As such, we cannot randomize the raw data because we would be unable to identify individuals in the results. Randomization must be done after entity linkage but before features are built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the information schema from selected database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the database schema containing input data to be randomized\n",
    "INPUT_SCHEMA = 'staging' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"SELECT * FROM information_schema.tables;\"\n",
    "tables = execute_sql(statement, dbname, user, host, password, port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the schema containing input datasets to be randomized. For this project, the schema is named staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tables = tables.table_name[tables.table_schema == INPUT_SCHEMA]\n",
    "input_tables = list(input_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First created the _randomized schema if it doesn't yet exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"CREATE SCHEMA IF NOT EXISTS {}_randomized;\".format(INPUT_SCHEMA)\n",
    "output = execute_sql(statement, dbname, user, host, password, port, isolation=True, results = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy over all the tables from original input schema, i.e. all tables in input_tables list.\n",
    "\n",
    "Randomize only a subset of these tables. Tables to be randomized are:    \n",
    "1) The set of tables from the staging schema that are referenced in the from\\_obj section of each of the yaml files in /config/features. These staging tables have been linked to entity identifiers and are the source data for feature generation.  \n",
    "2) Two labels tables from the staging schema that are referenced in config files: 'labels_casesonly_multiprior_win6mo_lab6mo' and 'labels_multiprior_win6mo_lab6mo'. Only randomizing the outcomes column in these labels tables to maintain the entity_id and outcome_date key pairing. This way, the states table does not need to be rerun.\n",
    "\n",
    "Note that table names beginning with \"staging.entity_\" are not included in the list because they contain mappings between entity identifiers and other data. These relationships must be maintained so should not be randomized.\n",
    "\n",
    "Write output to _randomized schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tables_to_randomize = [\n",
    "               'booking_addl_info', \n",
    "               'branch_lkup',\n",
    "               'case_booking',  \n",
    "               'case_dispo_lkup', \n",
    "               'case_flag_lkup',\n",
    "               'case_result_lkup',\n",
    "               'charge_lkup',\n",
    "               'feature_casesflag_info', \n",
    "               'feature_case_info', \n",
    "               'feature_case_results',\n",
    "               'feature_charge_info',\n",
    "               'labels_casesonly_multiprior_win6mo_lab6mo',\n",
    "               'orig_agency_lkup',\n",
    "               'labels_multiprior_win6mo_lab6mo'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for table_name in input_tables:\n",
    "    seed = 0 # initiate to 0, increment in each iteration\n",
    "    \n",
    "    print(\"Working on table {}\".format(table_name))\n",
    "    \n",
    "    if table_name in input_tables_to_randomize: # randomize before adding to _randomized schema\n",
    "        # Pull the table from original schema\n",
    "        print(\"\\tPulling table\")\n",
    "        statement = \"SELECT * FROM {}.{};\".format(INPUT_SCHEMA, table_name)\n",
    "        table = execute_sql(statement, dbname, user, host, password, port)\n",
    "\n",
    "        # Randomize the table\n",
    "        print(\"\\tRandomizing\")\n",
    "        if table_name in ['labels_casesonly_multiprior_win6mo_lab6mo','labels_multiprior_win6mo_lab6mo']:\n",
    "            randomized_table = randomize(table, do_not_randomize=['entity_id', 'outcome_date'], seed = seed)          \n",
    "        else:\n",
    "            randomized_table = randomize(table, seed = seed)\n",
    "\n",
    "        # Make a new table in _randomized schema\n",
    "        print(\"\\tUploading randomized version\")\n",
    "        statement = \"CREATE TABLE IF NOT EXISTS {0}_randomized.{1}_randomized (LIKE {0}.{1} INCLUDING ALL);\".format(INPUT_SCHEMA, table_name)\n",
    "        output = execute_sql(statement, dbname, user, host, password, port, isolation = True, results = False)\n",
    "\n",
    "        # Write results into new table\n",
    "        statement = \"SELECT COUNT(*) FROM {}_randomized.{}_randomized;\".format(INPUT_SCHEMA, table_name)\n",
    "        output = execute_sql(statement, dbname, user, host, password, port)\n",
    "        if output.iloc[0,0]>0: # do nothing if new table already contains data\n",
    "            print(\"\\t*****SKIPPING TABLE {}_randomized -- it already has data\".format(table_name))\n",
    "        else:\n",
    "            engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(user, password, host, port, dbname))\n",
    "            randomized_table.to_sql(table_name+'_randomized', engine, schema = '{}_randomized'.format(INPUT_SCHEMA), index = False, if_exists='append')\n",
    "    \n",
    "    else: # copy to _randomized schema as is, without randomizing\n",
    "        # Make a new table in _randomized schema\n",
    "        print(\"\\tUploading original version without randomizing\")\n",
    "        statement = \"CREATE TABLE IF NOT EXISTS {0}_randomized.{1} (LIKE {0}.{1} INCLUDING ALL);\".format(INPUT_SCHEMA, table_name)\n",
    "        output = execute_sql(statement, dbname, user, host, password, port, isolation = True, results = False)\n",
    "        \n",
    "        # Copy original data without randomizing\n",
    "        statement = \"SELECT COUNT(*) FROM {}_randomized.{};\".format(INPUT_SCHEMA, table_name)\n",
    "        output = execute_sql(statement, dbname, user, host, password, port)\n",
    "        if output.iloc[0,0]>0: # do nothing if new table already contains data\n",
    "            print(\"\\t*****SKIPPING TABLE {} -- it already has data\".format(table_name))\n",
    "        else:\n",
    "            statement = \"INSERT INTO {0}_randomized.{1} (SELECT * from {0}.{1});\".format(INPUT_SCHEMA, table_name)\n",
    "            output = execute_sql(statement, dbname, user, host, password, port, isolation = True, results = False)\n",
    "    seed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also randomized the labels, table labels_multiprior_win6mo_lab6mo; forgot to include in input_tables_to_randomize list when previous cell was run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Edit config files so that they point to the newly randomized raw schema\n",
    "\n",
    "Experiment config file being used on the project is experiment_config_multiprior.yaml. Also need to edit all the feature config files under config/features. New versions of config files with an \"_edited\" suffix will be produced and saved in \"features_randomized\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILES = [\n",
    "    'config/experiment_config_multiprior.yaml', \n",
    "    'config/features/booking_info.yaml',\n",
    "    'config/features/case_flags.yaml',\n",
    "    'config/features/case_info.yaml',\n",
    "    'config/features/case_results.yaml',\n",
    "    'config/features/charge_info.yaml',\n",
    "    'config/features/days_since.yaml',\n",
    "    'config/features/demos.yaml',\n",
    "    'config/features/multi_prior.yaml'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dictionary of text replacements to apply across every preprocessing & config file. This should include:\n",
    "- Schema and table names that have been changed, i.e. have a \"_randomized\" suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_MAP = {x:x+'_randomized' for x in input_tables_to_randomize}\n",
    "WORD_MAP[INPUT_SCHEMA+'.'] = INPUT_SCHEMA+'_randomized.'\n",
    "WORD_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply find+replace to every config file. Write feature config files to a new folder (config/features_randomized) that will later be referenced when running the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd config && mkdir features_randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename in CONFIG_FILES:\n",
    "#    new_filename = get_new_filename(filename, \"_edited\", {'features': 'features_randomized'})\n",
    "#    replace_words_in_file(filename, new_filename, WORD_MAP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Be sure to open the outputted files (with \"_edited\" suffix) to check that no unexpected replacements were made and that all necessary replacements are made. \n",
    "\n",
    "Model Group Key: We have to manually add a \"purpose\" model group key into the experiment config file. Including a \"purpose\" indicates that this experiment is for leakage detection and helps to distinguish it from existing experiments. Add \"purpose: leakage_detection\" under the user_metadata section, and add \"purpose\" under the model_group_keys section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Run an experiment with this new setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up randomized versions of the original schemas in the database and created a new config file for a randomized experiment, it's time to run the actual triage experiment. This is done in an aws ec2 instance.\n",
    "\n",
    "Make sure you have a database.yaml file, required by triage for modeling. See example [here](https://github.com/dssg/la_prosecutor/blob/master/example_database.yaml).\n",
    "\n",
    "Update run.py to reflect the particulars of this new config.\n",
    "- set PROJECT_PATH to the local directory where you want to store output\n",
    "- set n_processes to be less than the number of cores on your machine\n",
    "\n",
    "Run the following command: python run.py -v -c config/experiment_config_multiprior_edited.yaml -f features_randomized > log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Align the results of randomized experiment against original experiment\n",
    "\n",
    "Model group from the original, non-randomized experiment: 22  \n",
    "Model group from randomized experiment: 115. \n",
    "\n",
    "Note: experiment didn't finish running successfully, so for now we can only look at model_id 1116, 1120, 1124, and 1128 under model_group_id 115, with train_end_time between 2012-07-01 and 2014-01-01. This corresponds to models 22, 50, 76, and 100 under model_group_22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at model groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_ID_ORIG = 22\n",
    "GROUP_ID_RAND = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# original model group\n",
    "statement = \"SELECT * FROM results.models where model_group_id = {};\".format(GROUP_ID_ORIG)\n",
    "models_orig = execute_sql(statement, dbname, user, host, password, port)\n",
    "models_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomized model group \n",
    "statement = \"SELECT * FROM results.models WHERE model_group_id = {}\".format(GROUP_ID_RAND)\n",
    "model_groups = execute_sql(statement, dbname, user, host, password, port)\n",
    "model_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID_ORIG = 22 \n",
    "MODEL_ID_RAND = 1116 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statement = \"SELECT * FROM results.models where model_id = {};\".format(MODEL_ID_ORIG)\n",
    "models_rand = execute_sql(statement, dbname, user, host, password, port)\n",
    "models_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"SELECT * FROM results.models where model_id = {};\".format(MODEL_ID_RAND)\n",
    "models_rand = execute_sql(statement, dbname, user, host, password, port)\n",
    "models_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at predictions. Why does randomized model give the same score for every single person?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statement = \"SELECT * FROM results.predictions where model_id = {};\".format(MODEL_ID_ORIG)\n",
    "predictions_orig = execute_sql(statement, dbname, user, host, password, port)\n",
    "predictions_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"SELECT * FROM results.predictions where model_id = {};\".format(MODEL_ID_RAND)\n",
    "predictions_rand = execute_sql(statement, dbname, user, host, password, port)\n",
    "predictions_rand.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(predictions_orig.label_value)\n",
    "np.mean(predictions_rand.label_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there different number of observations in the two predictions tables?   \n",
    "Answer: Entity IDs have been reassigned since then, so it's okay that they don't match. However, the overall length of the lists of people should be the same, because they depend on the entity states which have not changed. Going to rerun staging pipeline to check for instabilities and investigate why prediction counts of original model do not match the state.\n",
    "\n",
    "For example: \n",
    "\n",
    "Model with train end time 2012-07-01:  \n",
    "Only 87,353 entities in common between original and random predictions.  \n",
    "There are 245,056 entities in original and not in random.  \n",
    "There are 238,644 entities in random and not in original.\n",
    "\n",
    "Model with train end time 2013-07-01:  \n",
    "Only 93,010 entities in common between original and random predictions.  \n",
    "There are 251,345 entities in original and not in random.  \n",
    "There are 244,662 entities in random and not in original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_orig.shape\n",
    "predictions_rand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_orig = set(predictions_orig.entity_id)\n",
    "entities_rand = set(predictions_rand.entity_id)\n",
    "entities_diff1 = entities_orig - entities_rand\n",
    "entities_diff2 = entities_rand - entities_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities_diff1)\n",
    "len(entities_diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities_orig.intersection(entities_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare train and test matrices between original and randomized model. Issues:   \n",
    "- There are additional feature columns in original model that are not in randomized model. Ivan/Erika have been working on fixing this.\n",
    "- Some features do not seem to have been created correctly in randomized model, such as the days_since... column which is only 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_matrix_orig = pd.read_csv('orig_models/7f857d499d4cb5f198ada6fbfc298932.csv', nrows=5000)\n",
    "train_orig_cols = set(train_matrix_orig.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_matrix_orig.shape\n",
    "train_matrix_orig.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_rand = pd.read_csv('experiment_output/matrices/ae5dfa11a6dc725a5ea9058a99eb599b.csv', nrows=5000)\n",
    "train_rand_cols = set(train_matrix_rand.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_matrix_rand.shape\n",
    "train_matrix_rand.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_orig.days_since_entity_id_50y_last_event_min.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_matrix_rand.days_since_entity_id_50y_last_event_min.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff1 = train_orig_cols - train_rand_cols\n",
    "diff1\n",
    "\n",
    "diff2 = train_rand_cols - train_orig_cols\n",
    "diff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the evaluation metrics for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"SELECT * FROM results.evaluations where model_id = {};\".format(MODEL_ID_ORIG)\n",
    "evaluations_orig = execute_sql(statement, dbname, user, host, password, port)\n",
    "evaluations_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"SELECT * FROM results.evaluations where model_id = {};\".format(MODEL_ID_RAND)\n",
    "evaluations_rand = execute_sql(statement, dbname, user, host, password, port)\n",
    "evaluations_rand.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
